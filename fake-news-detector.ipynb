{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Detector <a id='top'></a>\n",
    "\n",
    "## Using SageMaker\n",
    "\n",
    "_Using Amazon's SageMaker for Train | Deployment_\n",
    "\n",
    "---\n",
    "\n",
    "Now that we have explored our data in the previous [notebook](https://github.com/gtraskas/fake-news-detector/blob/master/data-exploration.ipynb), we are ready to use SageMaker to construct a complete ML classifier from end to end. Our goal will be to have a simple web page, which a user can use to enter some text from news. The web page will then send the text off to our deployed model, which will predict if it is fake or true.\n",
    "\n",
    "## General Outline\n",
    "\n",
    "1. [Import Libraries](#import)\n",
    "2. [Read in the Data](#read)\n",
    "3. [Prepare and Process the Data](#prepare)\n",
    "    1. [Split Data to Train/Test](#split)\n",
    "    2. [Clean and Tokenize Text](#clean)\n",
    "4. [Extract Features](#extract)\n",
    "5. [Build and Fit a Naive Bayes Model](#build)\n",
    "    1. [Evaluate Model](#evaluate)\n",
    "6. [Upload the Data to Amazon's S3](#upload)\n",
    "7. [Data Preprocessing for BlazingText](#blazingtext)\n",
    "8. [Deploy the Model](#deploy)\n",
    "    1. [Test the Model](#test)\n",
    "    2. [Clean Up](#clean-up)\n",
    "9. [Put the Model to Work](#work)\n",
    "    1. [Set up a Lambda Function](#lambda)\n",
    "    2. [Set up API Gateway](#api)\n",
    "10. [Deploy our Web App](#web)\n",
    "    1. [Delete the endpoint](#delete)\n",
    "    2. [Optional: Clean up](#opt-clean)\n",
    "\n",
    "**Note:** We will not be testing the model in its own step. We will do it by deploying the model and then using the deployed model by sending the test data to it. One of the reasons for doing this is so that we can make sure that our deployed model is working correctly before moving forward.\n",
    "\n",
    "## Import Libraries <a id='import'></a>\n",
    "\n",
    "In the next cells we will import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'joblib' from 'sklearn.externals' (C:\\Users\\LENOVO\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\externals\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-f598cb5ab9eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mhelpers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mread_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprocess_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m# import sagemaker\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\helpers.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msnowball\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSnowballStemmer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexternals\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'joblib' from 'sklearn.externals' (C:\\Users\\LENOVO\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\externals\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# Import libraries.\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import json\n",
    "import string\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "\n",
    "from helpers import read_data, process_text, prepare_data, extract_features\n",
    "\n",
    "# import sagemaker\n",
    "# from sagemaker import get_execution_role\n",
    "# from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "# from sagemaker.predictor import csv_serializer\n",
    "\n",
    "# Set global variables\n",
    "RANDOM_STATE = 5\n",
    "DIR = 'data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the Data <a id='read'></a>\n",
    "\n",
    "First we will read the data using the `read_data` function from the `helpers.py` file. This function read the downloaded data into `pandas` dataframes, then creates a column `label` to indicate if news is fake or true, concatenate the two datasets, shuffle data, and return the df. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data.\n",
    "df = read_data(DIR, RANDOM_STATE)\n",
    "\n",
    "# Show first rows.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect what other functions we have added in the `helpers.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pygmentize helpers.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare and Process the Data <a id='prepare'></a>\n",
    "\n",
    "Now, we will do some data processing. To begin with, we will join `title` and `text` columns into a single input structure and remove the columns `subject` and `date` (and the redundant `title` after the joining), since it wasn't found any useful features during the data exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join title and text into one column.\n",
    "df['text'] = df.title + \" \" + df.text\n",
    "\n",
    "# Remove useless columns.\n",
    "df.drop(columns=['subject', 'date', 'title'], axis=1, inplace=True)\n",
    "\n",
    "# Show the first rows.\n",
    "display(df.head())\n",
    "\n",
    "# Show an example of text.\n",
    "df.text[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data to Train/Test <a id='split'></a>\n",
    "\n",
    "Then, we will split the dataset into a training set and a testing set using 80% of the data for training purposes and the rest 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data to train and test datasets.\n",
    "train_X, test_X, train_y, test_y = train_test_split(df.text, df.label, test_size=0.2,\n",
    "                                                    random_state=RANDOM_STATE)\n",
    "\n",
    "print(\"Fake and True News (combined): train = {}, test = {}\".format(len(train_X), len(test_X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our training and testing sets unified and prepared, we should do a quick check and see an example of the data our model will be trained on. This is generally a good idea as it allows to see how each of the further processing steps affects the reviews and it also ensures that the data has been loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_X[3])\n",
    "print(train_y[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and Tokenize Text <a id='clean'></a>\n",
    "\n",
    "The steps in processing the text are:\n",
    "\n",
    "- Read a text file as a string of raw text.\n",
    "- Lower case all words, so that captialization is ignored (e.g., IndIcaTE is treated the same as Indicate).\n",
    "- Normalize numbers, replacing them with the text number.\n",
    "- Remove non-words, remove punctuation, and trim all white spaces (tabs, newlines, spaces) to a single space character.\n",
    "- Tokenize the raw text string into a list of words where each entry is a word. Tokenization is the act of breaking up a sequence of strings into pieces such as words, keywords, phrases, symbols and other elements called tokens. Tokens can be individual words, phrases or even whole sentences. In the process of tokenization, some characters like punctuation marks are discarded. The tokens become the input for another process like parsing and text mining. Then the words will be ready to be encoded as integers or floating point values for use as input to a machine learning algorithm, called feature extraction (or vectorization).\n",
    "- Use lemmatization or stemming to consolidate closely redundant words. For example, \"discount\", \"discounts\", \"discounted\" and \"discounting\" will be all replaced with \"discount\". Sometimes, the Stemmer actually strips off additional characters from the end, so \"include\", \"includes\", \"included\", and \"including\" are all replaced with \"includ\".\n",
    "- Remove stopwords. Stop words are so frequently used that for many tasks (but not all) they don't carry much information. Examples are \"any\", \"all\", \"what\", etc. NLTK has an inbuilt corpus of english stopwords that can be loaded and used.\n",
    "- Apply additional text preparation steps, such as normalizing links and emails: All https and http links will be replaced with the text \"link\" and all emails will be replaced with the text \"email\".\n",
    "\n",
    "The `process_text` method defined in `helpers.py` uses `BeautifulSoup` and `re` modules to 'normalize' text and uses also the `nltk` package to stem the text. As a check to ensure we know how everything is working, we wiil apply `process_text` to one example in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply process_text to an example.\n",
    "process_text(train_X[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything looks as expected. The next `prepare_data` method of the `helpers.py` caches the results. This is because performing this processing step can take a long time (about ~30 minutes in a MacBook Pro with 2.2 GHz 6-Core Intel Core i7). This way if we are unable to complete the notebook in the current session, we can come back without needing to process the data a second time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new 'tqdm' instance to time and estimate the progress of functions.\n",
    "tqdm.pandas()\n",
    "\n",
    "# Ensure directory exists.\n",
    "os.makedirs(DIR, exist_ok=True)\n",
    "\n",
    "# Preprocess data.\n",
    "train_X, test_X, train_y, test_y = prepare_data(train_X, test_X, train_y, test_y, cache_dir=DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Extract Features <a id='extract'></a>\n",
    "\n",
    "Before training machine learning algorithms, preprocessed text needs to be transformed into numerical data. This process is called feature extraction or vectorization. There are three popular feature extraction algorithms:\n",
    "\n",
    "- **Bag-of-Words**\n",
    "\n",
    "This algorithm is made up of two parts. First, it creates a dictionary from the entire corpus. Then, it transforms each text in the corpus as a vector of word occurences. This is called a \"bag\" because it disregards the order of the words within the text and focuses on content. A Bag of Words can be obtained using Sklearn's `CountVectorizer`.\n",
    "\n",
    "- **N-grams**\n",
    "\n",
    "A N-gram is a combination of N number of words treated as a single feature, as opposed to single word features in the Bag of Word. The idea is to extract contextual information and enrich data. For example the word \"good\" is always positive individually but can be negative when preceeded by \"not\". In certain cases, \"not good\" is an informative bigram. N-grams can also be extracted through sklearn's `CountVectorizer`, but with a specific parameter.\n",
    "\n",
    "- **Term Frequency - Inverse Document Frequency (Tfidf)**\n",
    "\n",
    "Rather than counting occurences, the TfIdf vectorizer computes an importance value for each word in its text and according the entire corpus. That value is the product of the TF and the IDF.\n",
    "\n",
    "TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
    "\n",
    "IDF(t) = log_e(Total number of documents / Number of documents with term t in it)\n",
    "\n",
    "The abovementioned steps of the Tfidf algorithm are automatized in Sklearn's `TfidfVectorizer` module.\n",
    "\n",
    "**Note:** We could use and test all the aforementioned algorithms using Sklearn's `Pipeline` package, which combines all the steps and simplifies our work, but this is out of the scope of this project. So, we will use the Bag-of-Words model, but enabling `ngram_range=(1, 3)`, so as to use more features (unigrams, bigrams, and trigrams).\n",
    "\n",
    "Later on when we will construct an endpoint, which processes a submitted text, we will need to make use of the `word_dict`, which we have created. As such, we will save it to a file now for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Bag of Words features for both training and test datasets.\n",
    "train_X, test_X, vocabulary = extract_features(train_X, test_X, 5000, cache_dir=DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check to make sure that things are working as intended examining one example text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell to examine one of the processed reviews to make sure everything is working as intended.\n",
    "print(train_X[5])\n",
    "print(len(train_X[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some n-grams from the dictionary.\n",
    "for key in sorted(vocabulary, key=vocabulary.get, reverse=True)[:20]:\n",
    "    print(key, ':', vocabulary[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything looks reasonable and the length of a text in the training set is 5000, which is the vocabulary size after extracting the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Fit a Naive Bayes Model <a id='build'></a>\n",
    "\n",
    "There are two options to build a model, either use the SageMaker built-in algorithms (like XGBoost) or customized ones or train the model in this instance using Sklearn's models. For the first option, we need to upload the data to S3 and then create the model, which comprises three objects:\n",
    "\n",
    " - Model Artifacts,\n",
    " - Training Code, and\n",
    " - Inference Code,\n",
    "\n",
    "each of which interact with one another.\n",
    "\n",
    "It is important to note the format of the data that we are saving as we will need to know it when we write the training code. In our case, each row of the dataset has the form `label`, `text`, where `text` is a sequence of integers representing the words in the review.\n",
    "\n",
    "The documentation for the algorithms in SageMaker requires that the saved datasets should contain no headers or index and that for the training and validation data, the **label should occur first** for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and fit the model.\n",
    "model = MultinomialNB()\n",
    "model.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model <a id='evaluate'></a>\n",
    "\n",
    "Use various metrics from Sklearn to evaluate our model on the test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make and save the predictions.\n",
    "predictions = model.predict(test_X)\n",
    "\n",
    "print(confusion_matrix(test_y, predictions))\n",
    "print(classification_report(test_y, predictions))\n",
    "plot_confusion_matrix(model, test_X, test_y, cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the Data to Amazon's S3 <a id='upload'></a>\n",
    "\n",
    "Amazon's S3 service allows us to store files that can be accessed by both the built-in training models such as the XGBoost model, as well as custom models. To do this, we need to split the training dataset into two parts, the data we will train the model with and a validation set. Then, we have to write those datasets to a file and upload the files to S3. In addition, we have to write the test set input to a file and upload the file to S3. This is so that we can use SageMakers Batch Transform functionality to test our model once we've fit it.\n",
    "\n",
    "It is important to note the format of the data that we are saving as we will need to know it when we write the training code. In our case, each row of the dataset has the form `label`, `text`, where `text` is a sequence of `5000` integers representing the words in the review.\n",
    "\n",
    "The documentation for the algorithms in SageMaker requires that the saved datasets should contain no headers or index and that for the training and validation data, the **label should occur first** for each sample.\n",
    "\n",
    "## Data Preprocessing for BlazingText <a id='blazingtext'></a>\n",
    "\n",
    "We need to preprocess the training data into space separated tokenized text format which can be consumed by the `BlazingText` algorithm. The class label(s) should be prefixed with `__label__` and it should be present in the same line along with the original sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data to train, validation, and test datasets.\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=RANDOM_STATE)\n",
    "df_train, df_valid = train_test_split(df_train, test_size=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "# Put label in first column.\n",
    "df_train = df_train[['label', 'text']]\n",
    "df_valid = df_valid[['label', 'text']]\n",
    "df_test = df_test[['label', 'text']]\n",
    "\n",
    "# Add __label__ to class as prefix.\n",
    "df_train.label = '__label__' + df_train.label.astype('str')\n",
    "df_valid.label = '__label__' + df_valid.label.astype('str')\n",
    "df_test.label = '__label__' + df_test.label.astype('str')\n",
    "\n",
    "# Clean and normalize text.\n",
    "df_train.text = df_train.text.progress_apply(process_text)\n",
    "df_valid.text = df_valid.text.progress_apply(process_text)\n",
    "df_test.text = df_test.text.progress_apply(process_text)\n",
    "\n",
    "# Show dfs.\n",
    "display(df_train.head())\n",
    "display(df_valid.head())\n",
    "display(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv.\n",
    "df_train.to_csv(os.path.join(DIR, 'news.train'), sep=' ', header=False, index=False)\n",
    "df_valid.to_csv(os.path.join(DIR, 'news.valid'), sep=' ', header=False, index=False)\n",
    "df_test.to_csv(os.path.join(DIR, 'news.test'), sep=' ', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to upload the data to the SageMaker default S3 bucket so that we can provide access to it while training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the current SageMaker session.\n",
    "session = sagemaker.Session()\n",
    "\n",
    "# Store the bucket.\n",
    "bucket = session.default_bucket()\n",
    "\n",
    "# S3 prefix (which folder will we use).\n",
    "prefix = 'fake-news-bt'\n",
    "\n",
    "# Upload the processed test, train and validation files,\n",
    "# which are contained in data directory to S3 using session.upload_data().\n",
    "test_location = session.upload_data(os.path.join(DIR, 'news.test'), key_prefix=prefix)\n",
    "val_location = session.upload_data(os.path.join(DIR, 'news.valid'), key_prefix=prefix)\n",
    "train_location = session.upload_data(os.path.join(DIR, 'news.train'), key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our current execution role is required when creating the model as the training\n",
    "# and inference code will need to access the model artifacts.\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to retrieve the location of the container, which is provided by Amazon for using XGBoost.\n",
    "# As a matter of convenience, the training and inference code both use the same container.\n",
    "container = get_image_uri(session.boto_region_name, 'blazingtext', 'latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we create a SageMaker estimator object for our model.\n",
    "bt_model = sagemaker.estimator.Estimator(container, # The location of the container we wish to use\n",
    "                                         role, # What is our current IAM Role\n",
    "                                         train_instance_count=1, # How many compute instances\n",
    "                                         train_instance_type='ml.c4.4xlarge', # What kind of compute instances\n",
    "                                         train_volume_size = 30,\n",
    "                                         train_max_run = 360000,\n",
    "                                         input_mode= 'File',\n",
    "                                         output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                         sagemaker_session=session)\n",
    "\n",
    "# And then set the algorithm specific parameters.\n",
    "bt_model.set_hyperparameters(mode=\"supervised\",\n",
    "                             epochs=10,\n",
    "                             min_count=2,\n",
    "                             learning_rate=0.05,\n",
    "                             vector_dim=10,\n",
    "                             early_stopping=True,\n",
    "                             patience=4,\n",
    "                             min_epochs=5,\n",
    "                             word_ngrams=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train = sagemaker.s3_input(s3_data=train_location, distribution='FullyReplicated',\n",
    "                                    content_type='text/plain')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data=val_location, distribution='FullyReplicated',\n",
    "                                         content_type='text/plain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_model.fit({'train': s3_input_train, 'validation': s3_input_validation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the Model <a id='deploy'></a>\n",
    "\n",
    "Once we construct and fit our model, SageMaker stores the resulting model artifacts and we can use those to deploy an endpoint (inference code). Deploying an endpoint is a lot like training the model with a few important differences. The first is that a deployed model doesn't change the model artifacts, so as you send it various testing instances the model won't change. Another difference is that since we aren't performing a fixed computation, as we were in the training step or while performing a batch transform, the compute instance that gets started stays running until we tell it to stop. This is important to note as if we forget and leave it running we will be charged the entire time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = bt_model.deploy(initial_instance_count = 1,instance_type = 'ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model <a id='test'></a>\n",
    "\n",
    "Now that we have deployed our endpoint, we can send the testing data to it and get back the inference results. We already did this earlier using the batch transform functionality of SageMaker, however, we will test our model again using the newly deployed endpoint so that we can make sure that it works properly and to get a bit of a feel for how the endpoint works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = pd.read_csv(os.path.join(DIR, 'news.test'), header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to define the batches.\n",
    "# From: https://stackoverflow.com/questions/8290397/how-to-split-an-iterable-in-constant-size-chunks\n",
    "def batch(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batches of 512 inputs for prediction and add them to a list.\n",
    "predictions = []\n",
    "for x in batch(test_X.iloc[:,0].str[12:-1].tolist(), 512):\n",
    "    payload = {\"instances\" : x}\n",
    "    prediction_batch = predictor.predict(json.dumps(payload))\n",
    "    prediction_batch = [int(prediction.get(\"label\")[0][9:]) for prediction in json.loads(prediction_batch)]\n",
    "    predictions.append(prediction_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten list.\n",
    "predictions = sum(predictions, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we check again to see what the accuracy of our model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(test_y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has very good accuracy on the unseen test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Up <a id='clean-up'></a>\n",
    "\n",
    "Now that we've determined that deploying our model works as expected, we are going to shut it down. Remember that the longer the endpoint is left running, the greater the cost and since we have a bit more work to do before we are able to use our endpoint with our simple web app, we should shut everything down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean only if you don't want to use later the Lambda function and API\n",
    "# to make predictions through a simple web app.\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put the Model to Work <a id='work'></a>\n",
    "\n",
    "This project's goal is to have our model deployed and then access it using a very simple web app. The intent is for this web app to take some user submitted data (a news text), send it off to our endpoint (the model) and then display the result as fake or true.\n",
    "\n",
    "However, there is a small catch. Currently the only way we can access the endpoint to send it data is using the SageMaker API. We can, if we wish, expose the actual URL that our model's endpoint is receiving data from, however, if we just send it data ourselves we will not get anything in return. This is because the endpoint created by SageMaker requires the entity accessing it have the correct permissions. So, we would need to somehow authenticate our web app with AWS.\n",
    "\n",
    "Having a website that authenticates to AWS seems a bit beyond the scope of this lesson so we will opt for an alternative approach. Namely, we will create a new endpoint which does not require authentication and which acts as a proxy for the SageMaker endpoint.\n",
    "\n",
    "As an additional constraint, we will try to avoid doing any data processing in the web app itself. Remember that when we constructed and tested our model we started with a movie review, then we simplified it by removing any html formatting and punctuation, then we constructed a bag of words embedding and the resulting vector is what we sent to our model. All of this needs to be done to our user input as well. Fortunately we can do all of this data processing in the backend, using Amazon's Lambda service.\n",
    "\n",
    "### Set up a Lambda Function <a id='lambda'></a>\n",
    "\n",
    "The first thing we are going to do is set up a Lambda function. This Lambda function will be executed whenever our public API has data sent to it. When it is executed it will receive the data, perform any sort of processing that is required, send the data (the text) to the SageMaker endpoint we've created and then return the result.\n",
    "\n",
    "#### Part A: Create an IAM Role for the Lambda function\n",
    "\n",
    "Since we want the Lambda function to call a SageMaker endpoint, we need to make sure that it has permission to do so. To do this, we will construct a role that we can later give the Lambda function.\n",
    "\n",
    "Using the AWS Console, we navigate to the **IAM** page and click on **Roles**. Then, we click on **Create role**. We make sure that the **AWS service** is the type of trusted entity selected and choose **Lambda** as the service that will use this role and then click **Next: Permissions**.\n",
    "\n",
    "In the search box we type `sagemaker` and select the check box next to the **AmazonSageMakerFullAccess** policy. Then, we click on **Next: Review**.\n",
    "\n",
    "Lastly, we give this role a name. We make sure we use a name that we will remember later on, for example `LambdaSageMakerRole`. Then, we click on **Create role**.\n",
    "\n",
    "#### Part B: Create a Lambda function\n",
    "\n",
    "Now it is time to actually create the Lambda function. In order to process the user provided input and send it to our endpoint we need to gather two pieces of information:\n",
    "\n",
    " - The name of the endpoint, and\n",
    " - the vocabulary object.\n",
    "\n",
    "We will copy these pieces of information to our Lambda function after we create it.\n",
    "\n",
    "To start, using the AWS Console, navigate to the AWS Lambda page and click on **Create a function**. When you get to the next page, make sure that **Author from scratch** is selected. Now, we name our Lambda function, using a name that we will remember later on, for example `fake_news_func`. We make sure that the latest **Python** runtime is selected and then choose the role that we created in the previous part. Then, we click on **Create Function**.\n",
    "\n",
    "On the next page we will see some information about the Lambda function we've just created. If we scroll down we should see an editor in which we can write the code that will be executed when our Lambda function is triggered. Collecting the code we wrote above to process text and adding it to the provided example `lambda_handler` we arrive at the following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# We need to use the low-level library to interact with SageMaker since the SageMaker API\n",
    "# is not available natively through Lambda.\n",
    "import boto3\n",
    "\n",
    "# And we need the following libraries to do some of the data processing.\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "\n",
    "# Create a function to process text.\n",
    "def process_text(text):\n",
    "    # Normalize links replacing them with the str 'link'.\n",
    "    text = re.sub('http\\S+', 'link', text)\n",
    "\n",
    "    # Normalize numbers replacing them with the str 'number'.\n",
    "    text = re.sub('\\d+', 'number', text)\n",
    "\n",
    "    # Normalize emails replacing them with the str 'email'.\n",
    "    text = re.sub('\\S+@\\S+', 'email', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove punctuation.    \n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Remove whitespaces.\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Convert all letters to lower case.\n",
    "    text = text.lower()\n",
    "\n",
    "    # Split text into words.\n",
    "    words = text.split()\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "runtime= boto3.client('runtime.sagemaker')\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    data = event['body']\n",
    "    sentence = process_text(data)\n",
    "\n",
    "    try:\n",
    "        payload = {\"instances\" : sentence}\n",
    "\n",
    "        response = runtime.invoke_endpoint(EndpointName='blazingtext-2020-06-15-07-32-18-142',\n",
    "                                            ContentType='application/json',\n",
    "                                            Body=json.dumps(payload))\n",
    "\n",
    "        result = json.loads(response['Body'].read().decode())\n",
    "        # prob = []\n",
    "        labels = []\n",
    "        for label in result[0]['label']:\n",
    "            labels.append(label[9:])\n",
    "        print(\"DATA\", data)\n",
    "        print(\"SENTENCE\", sentence)\n",
    "        return {\n",
    "            'statusCode' : 200,\n",
    "            'headers' : { 'Content-Type' : 'text/plain', 'Access-Control-Allow-Origin' : '*' },\n",
    "            'body' : str(labels[0])\n",
    "    }\n",
    "        # return {'statusCode': 200, 'body': str(labels[0])}\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return {'statusCode': 400,\n",
    "                'body': json.dumps({'error_message': 'Unable to generate tag.'})}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have copy and pasted the code above into the Lambda code editor, we replace the `**ENDPOINT NAME HERE**` portion with the name of the endpoint that we deployed earlier. We can determine the name of the endpoint using the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we need to copy the vocabulary dictionary to the appropriate place in the code at the beginning of the lambda_handler method. The cell below prints out the vocabulary dict in a way that is easy to copy and paste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(str(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have added the endpoint name to the Lambda function, we click on **Save**. Our Lambda function is now up and running. Next we need to create a way for our web app to execute the Lambda function.\n",
    "\n",
    "### Set up API Gateway <a id='api'></a>\n",
    "\n",
    "Now that our Lambda function is set up, it is time to create a new API using API Gateway that will trigger the Lambda function we have just created.\n",
    "\n",
    "Using AWS Console, we navigate to **Amazon API Gateway** and then click on **Get started**.\n",
    "\n",
    "On the next page, we make sure that **New API** is selected and give the new api a name, for example, `fake_news_web_app`. Then, we click on **Create API**.\n",
    "\n",
    "Now we have created an API, however it doesn't currently do anything. What we want it to do is to trigger the Lambda function that we created earlier.\n",
    "\n",
    "Select the **Actions** dropdown menu and click **Create Method**. A new blank method will be created, we select its dropdown menu and select **POST** and then click on the check mark beside it.\n",
    "\n",
    "For the integration point, we make sure that **Lambda Function** is selected and we click on the **Use Lambda Proxy integration**. This option makes sure that the data that is sent to the API is then sent directly to the Lambda function with no processing. It also means that the return value must be a proper response object as it will also not be processed by API Gateway.\n",
    "\n",
    "We type the name of the Lambda function we created earlier into the **Lambda Function** text entry box and then click on **Save**. We click on **OK** in the pop-up box that then appears, giving permission to API Gateway to invoke the Lambda function we created.\n",
    "\n",
    "The last step in creating the API Gateway is to select the **Actions** dropdown and we click on **Deploy API**. We will need to create a new Deployment stage and name it anything we like, for example `prod`.\n",
    "\n",
    "We have now successfully set up a public API to access your SageMaker model. We make sure to copy or write down the URL provided to invoke our newly created public API as this will be needed in the next step. This URL can be found at the top of the page, highlighted in blue next to the text **Invoke URL**.\n",
    "\n",
    "## Deploy our Web App <a id='web'></a>\n",
    "\n",
    "Now that we have a publicly available API, we can start using it in a web app. For our purposes, we have provided a simple static html file, which can make use of the public api we created earlier.\n",
    "\n",
    "In the `website` folder there should be a file called `index.html`. We edit this file in a text editor of our choice:\n",
    "\n",
    "- There is a line which contains **\\*\\*REPLACE WITH PUBLIC API URL\\*\\***. We replace this string with the url that we wrote down in the last step and then save the file.\n",
    "\n",
    "Now, if we open `index.html` on our local computer, our browser will behave as a local web server and we can use the provided site to interact with our SageMaker model.\n",
    "\n",
    "If we'd like to go further, we can host this html file anywhere we'd like, for example using github or hosting a static site on Amazon's S3. Once we have done this we can share the link with anyone we'd like and have them play with it too.\n",
    "\n",
    "> **Important Note** In order for the web app to communicate with the SageMaker endpoint, the endpoint has to actually be deployed and running. This means that we are paying for it. Make sure that the endpoint is running when we want to use the web app but that we shut it down when we don't need it, otherwise we will end up with a surprisingly large AWS bill.\n",
    "\n",
    "### Delete the endpoint <a id='delete'></a>\n",
    "\n",
    "Remember to always shut down our endpoint if we are no longer using it. We are charged for the length of time that the endpoint is running so if we forget and leave it on we could end up with an unexpectedly large bill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Clean up <a id='opt-clean'></a>\n",
    "\n",
    "The default notebook instance on SageMaker doesn't have a lot of excess disk space available. As we continue to complete and execute notebooks we will eventually fill up this disk space, leading to errors, which can be difficult to diagnose. Once we are completely finished using a notebook it is a good idea to remove the files that we created along the way. Of course, we can do this from the terminal or from the notebook hub if we would like. The cell below contains some commands to clean up the created files from within the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we will remove all of the files contained in the data directory.\n",
    "!rm $data_dir/*\n",
    "\n",
    "# Then we delete the directory itself.\n",
    "!rmdir $data_dir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
